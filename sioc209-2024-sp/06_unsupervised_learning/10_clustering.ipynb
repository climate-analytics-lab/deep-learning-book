{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning for Geo/Environmental sciences\n",
    "\n",
    "<center><img src=\"../logo_2.png\" alt=\"logo\" width=\"500\"/></center>\n",
    "\n",
    "<em>*Created with ChapGPT</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/climate-analytics-lab/sioc209-2024-sp/blob/main/sioc209-2024-sp/06_unsupervised_learning/10_clustering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 11: Unsupervised Learning and Clustering\n",
    "\n",
    " - [Unsupervised Learning](#Unsupervised-Learning)\n",
    " - [K-means Clustering](#K-means-Clustering)\n",
    " - [Hierarchical Clustering](#Hierarchical-Clustering)\n",
    " - [Self-Organizing Maps](#Self-Organizing-Maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "We have so far discussed supervised learning, where we have a dataset with input features and corresponding labels. The goal is to learn a mapping from the input features to the labels. But what if you don't have labels for your data?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is where unsupervised learning comes in. Unsupervised learning is a type of machine learning that looks for previously undetected patterns and structure in a dataset with no pre-existing labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are broadly two types of unsupervised learning: clustering and dimensionality reduction. In this lecture, we will focus on clustering and we'll cover dimensionality reduction in the next lecture (and generative models the lecture after that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../01_intro/_images/ML_overview.png\" alt=\"logo\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "Clustering is a type of unsupervised learning that groups data points together based on similarities. The goal is to create groups that are more similar to each other than they are to data points in other groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Clustering is used in a variety of applications, such as:\n",
    "\n",
    "- Finding regimes / types of behavior in data\n",
    "- Item clustering\n",
    "- Anomaly detection\n",
    "- Image segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "There are a wide variety of clustering algorithms with different strengths and weaknesses. This example from sci-kit learn shows how different clustering algorithms perform on different types of data:\n",
    "\n",
    "<center><img src=\"_images/cluster_comparison.png\" alt=\"clustering comparison\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this lecture, we will cover two of the most common clustering algorithms: K-means and hierarchical clustering. We will also briefly discuss self-organizing maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## k-means Clustering\n",
    "\n",
    "k-means clustering is one of the more popular clustering techniques in Earth Science due to its relative simplicity. The aim is to group $ N $ observations into $ k $ clusters in which each observation belongs to the cluster with the nearest center. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The optimal distribution of the cluster centers is the distribution that minimizes the within-cluster sum of squares (i.e., the sum of the distance functions of each point to the cluster center). Mathematically, this is written as:\n",
    "\n",
    "$\\\\arg \\min_S = \\sum_{i=1}^{k} \\sum_{x \\in S_i} \\| x - \\mu_i \\|^2 $\n",
    "\n",
    "where $ S = S_1, S_2, \\ldots, S_k $ denotes the set of $ k $ clusters, $ x $ denotes the set of observations, $ \\mu_i $ denotes the center of cluster $ S_i $ defined as the mean of all points in $ S_i $, and $ \\arg \\min $ specifies that we are looking for the minimum over varying options of $ S $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## k-means Clustering\n",
    "\n",
    "While the idea behind k-means is straightforward, actually computing the most optimal solution is computationally expensive, especially for large datasets. Most implementations of k-means use a heuristic approach to find a local minimum of the within-cluster sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Thus, in practice, one typically runs the k-means clustering algorithm many times (with each instance starting with a unique initial guess) and takes the iteration that produces the smallest minimum within-cluster sum of squares (i.e., the equation above). This also allows one to assess the stability of the clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## k-means Clustering\n",
    "\n",
    "The standard algorithm for k-means clustering is Lloydâ€™s algorithm, which begins with an initial guess of the cluster centers $ \\mu_i $ and then iteratively refines this guess until it converges. \n",
    "\n",
    "The following figure shows an example of this iterative process.\n",
    "\n",
    "<center><img src=\"_images/lloyds_algorithm.png\" alt=\"Lloyd's algorithm\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This section, and the above example image is modified from Libby Barnes' course notes on clustering: https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Example application to Old Faithful eruptions\n",
    "\n",
    "Let's apply k-means clustering to the Old Faithful dataset. The dataset contains the duration of the Old Faithful geyser eruptions and the waiting time until the next eruption. We will use the `KMeans` class from `sklearn.cluster` to perform the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"_data/old_faithful_eruptions.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example application to Old Faithful eruptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "df.plot.scatter(x='duration', y='delay',  cmap='viridis', ax=axs[0],\n",
    "                c=KMeans(n_clusters=2, random_state=42).fit_predict(df))\n",
    "df.plot.scatter(x='duration', y='delay', cmap='viridis', ax=axs[1],\n",
    "                c=KMeans(n_clusters=5, random_state=42).fit_predict(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## k-means Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "k-means clustering is a simple and popular clustering algorithm that groups data points into $ k $ clusters based on their similarity. The algorithm aims to minimize the within-cluster sum of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To apply k-means clustering, one key decision must be made: How many clusters do you want? \n",
    " -  This is a decision that must be made by the user and is not determined by the clustering algorithm. One will obtain different results based on the choice of $ k $ as seen in the example above. \n",
    "\n",
    " - Quantities such as the gap statistic, the elbow method, and the silhouette score can be used to help determine the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "See e.g. https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heirarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hierarchical clustering is another popular clustering technique that does not require the user to specify the number of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The algorithm works by grouping data points into a tree of clusters. The tree is called a dendrogram and is created by iteratively merging the two closest clusters until all data points are in one cluster. The user can then choose how many clusters they want by cutting the dendrogram at the desired level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heirarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are two types of hierarchical clustering: agglomerative and divisive. \n",
    "\n",
    " - Agglomerative clustering is the more popular of the two and works by starting with each data point as its own cluster and then iteratively merging the two closest clusters until all data points are in one cluster. \n",
    " - Divisive clustering works in the opposite way by starting with all data points in one cluster and then iteratively splitting the cluster until each data point is in its own cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Heirarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Note, that unlike k-means clustering we cannot assign new data points to clusters after the clustering is complete. This is because the clustering is based on the structure of the dendrogram and not on the cluster centers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Put another way: There is no notion of a cluster center in hierarchical clustering so we can only assign new data points to clusters by re-running the clustering algorithm with the new data points included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at an example of hierarchical clustering applied to a synthetic dataset with three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X, _ = make_blobs(n_samples=10, centers=3, cluster_std=0.60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use the `scipy` library to perform hierarchical clustering. The `linkage` function computes the hierarchical clustering of the input data and returns a linkage matrix that encodes the hierarchical clustering. \n",
    "\n",
    "We specify the method as `ward` to use the Ward variance minimization algorithm. This algorithm minimizes the sum of squared differences within all clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the linkage matrix\n",
    "Z = linkage(X, 'ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `dendrogram` function then plots the dendrogram of the hierarchical clustering. The y-axis represents the distance between clusters. The height of the dendrogram at each merge represents the distance between the two clusters that are merged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `colors_threshold` argument allows us to specify the distance at which we want to cut the dendrogram to obtain different clusters. The dendrogram will then color the clusters accordingly. We take those colors and use them to plot the data points with the corresponding cluster colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the dendrogram and get the color list\n",
    "dendro = dendrogram(Z, color_threshold=2, ax=ax1)\n",
    "ax1.set_title('Dendrogram')\n",
    "\n",
    "# Extract the color list from the dendrogram\n",
    "color_list = [x for _, x in sorted(zip(dendro['leaves'], dendro['leaves_color_list']))]\n",
    "\n",
    "# Plot the final clustering with matching colors\n",
    "for i, (x, y) in enumerate(X):\n",
    "    ax2.scatter(x, y, color=color_list[i])\n",
    "    ax2.text(x, y, str(i), fontsize=12, ha='right')\n",
    "\n",
    "ax2.set_title('Final Clusters (matched colors)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Agglomerative clustering is useful when your data is not well separated into distinct clusters. It is also useful when you want to see the relationships between clusters. However, it is computationally expensive and can be slow for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Self-Organizing Maps (SOM)\n",
    "\n",
    "Self-Organizing Maps (SOM), introduced by Teuvo Kohonen in the 1980s, are a type of artificial neural network trained using unsupervised learning \n",
    "to produce a low-dimensional (typically two-dimensional) representation of the input space, known as a map.\n",
    "SOMs are particularly useful for visualizing high-dimensional data and clustering similar data points together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A SOM consists of a grid of nodes (neurons), each of which is associated with a weight vector of the same dimension as the input data. \n",
    "The training process involves iteratively updating the weights to preserve the topological properties of the input space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Self-Organizing Maps (SOM)\n",
    "\n",
    "Given some intial weight vectors $ w_j $, the SOM algorithm proceeds as follows:\n",
    "   - Find the best matching unit (BMU) $ c $, which is the node with the smallest distance to $ x $: $      c = \\arg\\min_j \\| x - w_j \\|      $\n",
    "   \n",
    "   - Update the weight vectors of the BMU and its neighbors to move them closer to $ x $:\n",
    "      -    $      w_j(t+1) = w_j(t) + \\eta(t) \\cdot h_{c,j}(t) \\cdot (x - w_j(t)) $\n",
    "      -   where $ \\eta(t) $ is the learning rate and $ h_{c,j}(t) $ is the neighborhood function, which decreases with time and distance from the BMU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This visualizations shows the updating of the weights in a SOM:\n",
    "\n",
    "<center><img src=\"_images/train_som.gif\" alt=\"SOM\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This makes it clear that what the SOM is learning is the topology of the input space - the mapping from the high-dimensional input space to the low-dimensional map space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "The SOM algorithm is a form of competitive learning, where nodes compete to represent the input data. The winning node (BMU) and its neighbors are updated to better represent the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at an example of applying a SOM to a synthetic dataset with three clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# We need the minisom package for this example\n",
    "# !pip install minisom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minisom import MiniSom\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data with underlying clusters\n",
    "n_samples = 300\n",
    "n_features = 3\n",
    "n_clusters = 4\n",
    "data, true_labels = make_blobs(n_samples=n_samples, centers=n_clusters, n_features=n_features, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We initialize the SOM with a grid of nodes and random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the SOM\n",
    "som_size = (5, 5)  # 10x10 grid\n",
    "som = MiniSom(som_size[0], som_size[1], n_features, sigma=0.2, learning_rate=0.5)\n",
    "som.random_weights_init(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, we train the SOM by iterating over the input data and updating the weights of the BMU and its neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som.train_random(data, 1000)  # 500 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that the SOM is trained, we can cluster the weights of the nodes to identify similar regions in the input space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights of the SOM neurons\n",
    "weights = som.get_weights().reshape(-1, n_features)\n",
    "\n",
    "# Apply k-means clustering to the SOM neurons\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=1)\n",
    "kmeans.fit(weights)\n",
    "labels = kmeans.labels_.reshape(som_size[0], som_size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's visualize the SOM and the clustered weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the SOM with the k-means clusters\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pcolor(labels, cmap='coolwarm', edgecolors='k', linewidths=1)\n",
    "plt.colorbar()\n",
    "plt.title(\"K-means Clustering of SOM Neurons\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then we can assign each input data point to the cluster corresponding to the nearest node. The nearest node is determined by the Euclidean distance between the input data point and the weight vector of the node and is sometimes called the 'winning' node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the cluster for each data point\n",
    "data_clusters = np.zeros(n_samples, dtype=int)\n",
    "for i, x in enumerate(data):\n",
    "    bmu = som.winner(x)\n",
    "    data_clusters[i] = labels[bmu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And plot the data points with the corresponding cluster colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), sharey=True, sharex=True)\n",
    "\n",
    "ax1.scatter(data[:, 0], data[:, 1], c=true_labels, cmap='viridis', edgecolor='k')\n",
    "ax1.set_title(\"Original Data with True Labels\")\n",
    "\n",
    "ax2.scatter(data[:, 0], data[:, 1], c=data_clusters, cmap='viridis', edgecolor='k')\n",
    "ax2.set_title(\"Original Data with Predicted Clusters\")\n",
    "ax1.set_xlabel(\"Feature 1\")\n",
    "ax2.set_xlabel(\"Feature 1\")\n",
    "ax1.set_ylabel(\"Feature 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SOM Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, this gives you a sense of how SOMs can be used for clustering and visualization of high-dimensional data. They are particularly useful for identifying patterns and relationships in complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the next lecture, we will cover dimensionality reduction techniques, such as t-SNE and VAEs, which can be used in conjunction with clustering algorithms to visualize high-dimensional data."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:sio209_dev]",
   "language": "python",
   "name": "conda-env-sio209_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
